# HFæ¨¡å‹ç¼–è¯‘ä¸ºTensorRTæ¨¡å‹è¯´æ˜

## 1. HFæ¨¡å‹è½¬ä¸ºPyTorchæ¨¡å‹

Huggingfaceçš„æ¨¡å‹æ— æ³•ç›´æ¥è½¬ä¸ºonnxï¼ˆä»…æœ‰éƒ¨åˆ†æ”¯æŒï¼‰ï¼Œå¦‚æœæ˜¯huggingfaceçš„æ¨¡å‹ï¼Œéœ€è¦å…ˆè½¬æˆPyTorch

Huggingfaceæœ¬èº«çš„æ¨¡å‹å…¶å®å°±æ˜¯åŸºäºPyTorchçš„ï¼Œä½†æ˜¯æ ¼å¼ä¸ç®—é€šç”¨ã€‚

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import AutoModel
import torch

# convert huggingface model to pytorch model

model = AutoModelForSeq2SeqLM.from_pretrained("../../opus-mt-zh-en")
print("model from huggingface loaded, now eval")
model.eval()
print("eval finished,now convert")
torch.save(model, 'opus-mt-zh-en.pt')
print("convert finished")
```

## 2. PyTorchæ¨¡å‹è½¬ä¸ºONNXæ¨¡å‹

### 2.1 ä½¿ç”¨torch.onnx.exportå‡½æ•°ï¼ˆå¤±è´¥ï¼‰

è¿™é‡Œé¢æœ‰å‡ ç‚¹éœ€è¦æ³¨æ„ï¼š

1. è½¬æ¢å‰éœ€è¦è°ƒç”¨evalå‡½æ•°
2. å¦‚æœæ¨¡å‹å¾ˆå¤§ï¼Œé‚£ä¹ˆè½¬æ¢å‡ºæ¥çš„æ–‡ä»¶ä¼šå¾ˆå¤šï¼Œå› ä¸ºå­˜å‚¨onnxçš„æ–‡ä»¶å¤§å°ä¸å…è®¸è¶…è¿‡2Gï¼Œè¿™æ˜¯æ­£å¸¸çš„
3. exportå‡½æ•°ä¸­ï¼Œ`input_names=["input"], output_names=["output"]`æ˜¯è‡ªå·±å®šä¹‰çš„ã€‚`dynamic_axes={'input': {1:'tokenlength'},'output' : [0,1]}`æ˜¯ç”¨æ¥å®šä¹‰è¾“å…¥è¾“å‡ºçš„å¯å˜é•¿åº¦çš„ã€‚æ¯”å¦‚ï¼Œå¯¹äºæ–‡æœ¬é—®ç­”æ¨¡å‹ï¼Œæ¯æ¬¡è¾“å…¥çš„é•¿åº¦éƒ½ä¸ç»Ÿä¸€ï¼Œå¦‚æœæŒ‰ç…§è¿™æ¬¡è½¬æ¢çš„æ¨ç†ï¼Œé‚£ä¹ˆinputçš„ç»´åº¦å°±å›ºå®šäº†ï¼Œè¿™æ ·å½“å†æ¬¡åŠ è½½æ¨¡å‹æ¨ç†çš„æ—¶å€™ï¼Œå°±ä¼šæŠ¥é•¿åº¦ä¸ä¸€è‡´çš„é”™è¯¯ï¼Œæ‰€ä»¥è¿™é‡Œè¦è®¾ç½®ä¸ºå¯å˜é•¿åº¦ã€‚å…¶ä¸­ï¼Œ1è¡¨ç¤ºä¸‰ç»´å¼ é‡çš„ç¬¬äºŒç»´æ˜¯å¯å˜çš„ã€‚`'output' : [0,1]`è¡¨ç¤ºoutputå¼ é‡ï¼ˆåˆ—è¡¨ï¼‰çš„ç¬¬ä¸€ç»´å’Œç¬¬äºŒç»´ï¼Œéƒ½æ˜¯å¯å˜çš„ã€‚

```python
from transformers import AutoTokenizer
from transformers import AutoModel
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

src_text = "ä»æ—¶é—´ä¸Šçœ‹ï¼Œä¸­å›½ç©ºé—´ç«™çš„å»ºé€ æ¯”å›½é™…ç©ºé—´ç«™æ™š20å¤šå¹´ã€‚"
tokenizer = AutoTokenizer.from_pretrained("../../opus-mt-zh-en")
encoder_input_ids = tokenizer.encode(src_text, return_tensors="pt", 
                                     padding=True)
print(f"è¾“å…¥ç»´åº¦: {encoder_input_ids.shape}")
# decoder_input_ids = torch.randint_like(encoder_input_ids, 0, 
#                                        20000, dtype=torch.long)
# decoder_input_ids =  ['In terms of time, the Chinese space 
#                        station was built#  more than 20 years 
#                        later than the International Space Station.']
decoder_input_ids = tokenizer.encode('In terms of time, the aaaaaaaaab', 
                                     return_tensors="pt", padding=True) 
# decoder_input_ids = torch.randint_like(encoder_input_ids, 0, 20000, 
#                                        dtype=torch.long)
# decoder_input_ids = torch.ones( 1,  19)
print(f"è¾“å‡ºç±»å‹: {decoder_input_ids.type}")
print(f"è¾“å‡ºç»´åº¦: {decoder_input_ids.shape}")
# # æ„é€ encoderç«¯å’Œdecoderç«¯çš„attention mask
# encoder_attn_mask = torch.ones(1, encoder_input_ids.shape[1], 
#                                dtype=torch.long)  
# decoder_attn_mask = torch.ones(1, decoder_input_ids.shape[1], 
#                                dtype=torch.long)

pt_model = torch.load('opus-mt-zh-en.pt')
# print(f"è¾“å…¥å±‚: {pt_model.model.shared}")
# print(f"è¾“å‡ºå±‚: {pt_model.lm_head}")
pt_model.eval()
# output = pt_model(input_ids=input_ids, 
#                   decoder_input_ids=decoder_input_ids)
# print(output)

torch.onnx.export(
    pt_model, 
    (encoder_input_ids,decoder_input_ids), 
    "opus-mt-zh-en.onnx",
    input_names=['encoder_input', 'decoder_input'], 
    output_names=["output"],
    opset_version=11,
    dynamic_axes={'input': {0:'tokenlength'},
                  'output' : {0:'outputlength'}}
    ) # è¾“å…¥çš„ç¬¬1ä¸ºå¯å˜



import torch.onnx 
# è½¬æ¢çš„onnxæ ¼å¼çš„åç§°ï¼Œæ–‡ä»¶åç¼€éœ€ä¸º.onnx
onnx_file_name = "xxxxxx.onnx"
# æˆ‘ä»¬éœ€è¦è½¬æ¢çš„æ¨¡å‹ï¼Œå°†torch_modelè®¾ç½®ä¸ºè‡ªå·±çš„æ¨¡å‹
model = torch_model
# åŠ è½½æƒé‡ï¼Œå°†model.pthè½¬æ¢ä¸ºè‡ªå·±çš„æ¨¡å‹æƒé‡
# å¦‚æœæ¨¡å‹çš„æƒé‡æ˜¯ä½¿ç”¨å¤šå¡è®­ç»ƒå‡ºæ¥ï¼Œæˆ‘ä»¬éœ€è¦å»é™¤æƒé‡ä¸­å¤šçš„module. å…·ä½“æ“ä½œå¯ä»¥è§5.4èŠ‚
model = model.load_state_dict(torch.load("model.pth"))
# å¯¼å‡ºæ¨¡å‹å‰ï¼Œå¿…é¡»è°ƒç”¨model.eval()æˆ–è€…model.train(False)
model.eval()
# dummy_inputå°±æ˜¯ä¸€ä¸ªè¾“å…¥çš„å®ä¾‹ï¼Œä»…æä¾›è¾“å…¥shapeã€typeç­‰ä¿¡æ¯ 
batch_size = 1 # éšæœºçš„å–å€¼ï¼Œå½“è®¾ç½®dynamic_axesåå½±å“ä¸å¤§
dummy_input = torch.randn(batch_size, 1, 224, 224, requires_grad=True) 
# è¿™ç»„è¾“å…¥å¯¹åº”çš„æ¨¡å‹è¾“å‡º
output = model(dummy_input)
# å¯¼å‡ºæ¨¡å‹
torch.onnx.export(model,        # æ¨¡å‹çš„åç§°
                  dummy_input,   # ä¸€ç»„å®ä¾‹åŒ–è¾“å…¥
                  onnx_file_name,   # æ–‡ä»¶ä¿å­˜è·¯å¾„/åç§°
                  export_params=True,        #  å¦‚æœæŒ‡å®šä¸ºTrueæˆ–é»˜è®¤, å‚æ•°ä¹Ÿ
                                             #  ä¼šè¢«å¯¼å‡º. å¦‚æœä½ è¦å¯¼å‡ºä¸€ä¸ªæ²¡è®­
                                             #  ç»ƒè¿‡çš„å°±è®¾ä¸º False.

                  opset_version=10,          # ONNX ç®—å­é›†çš„ç‰ˆæœ¬ï¼Œå½“å‰å·²æ›´æ–°
                                             #  åˆ°15
                  do_constant_folding=True,  # æ˜¯å¦æ‰§è¡Œå¸¸é‡æŠ˜å ä¼˜åŒ–
                  input_names = ['input'],   # è¾“å…¥æ¨¡å‹çš„å¼ é‡çš„åç§°
                  output_names = ['output'], # è¾“å‡ºæ¨¡å‹çš„å¼ é‡çš„åç§°
                  # dynamic_axeså°†batch_sizeçš„ç»´åº¦æŒ‡å®šä¸ºåŠ¨æ€ï¼Œ
                  # åç»­è¿›è¡Œæ¨ç†çš„æ•°æ®å¯ä»¥ä¸å¯¼å‡ºçš„dummy_inputçš„batch_sizeä¸åŒ
                  dynamic_axes={'input' : {0 : 'batch_size'},    
                                'output' : {0 : 'batch_size'}})
```

**å¤±è´¥åŸå› ï¼š** å¯¹äº**Seq2Seq**ä»»åŠ¡çš„**Encoder-Decoder**æ¶æ„æ¨¡å‹ï¼Œéœ€è¦ç”¨pytorchå°†æ¨¡å‹æ‹†åˆ†ä¸ºEncoderå’ŒDecoderä¸¤ä¸ªéƒ¨åˆ†ï¼Œç„¶åè¿›è¡Œå¯¼å‡ºï¼Œæ“ä½œéš¾åº¦è¾ƒå¤§ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹T5å¯¼å‡ºä»£ç 

`https://github.com/Ki6an/fastT5/blob/8dda859086af631a10ad210a5f1afdec64d49616/fastT5/onnx_exporter.py#L45`

### 2.2 ä½¿ç”¨optimum-cliå¯¼å‡º

```bash
#! /bin/bash
pip install optimum[exporters]
# pip install --upgrade diffusers
# optimum-cli export onnx --model opus-mt-zh-en onnx/   # ä» ğŸ¤— Hub å¯¼å‡ºæ£€æŸ¥ç‚¹çš„è¿‡ç¨‹ã€‚
optimum-cli export onnx --model ../../opus-mt-zh-en --task text2text-generation onnx/
```

### 2.3 éªŒè¯

æ¨¡å‹æ ¡éªŒ

```python
import onnx
# æˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¼‚å¸¸å¤„ç†çš„æ–¹æ³•è¿›è¡Œæ£€éªŒ
try:
    # å½“æˆ‘ä»¬çš„æ¨¡å‹ä¸å¯ç”¨æ—¶ï¼Œå°†ä¼šæŠ¥å‡ºå¼‚å¸¸
    onnx.checker.check_model(self.onnx_model)
except onnx.checker.ValidationError as e:
    print("The model is invalid: %s"%e)
else:
    # æ¨¡å‹å¯ç”¨æ—¶ï¼Œå°†ä¸ä¼šæŠ¥å‡ºå¼‚å¸¸ï¼Œå¹¶ä¼šè¾“å‡ºâ€œThe model is valid!â€
    print("The model is valid!")
```

# Tritonä½¿ç”¨æŒ‡å—

## Part1 å¦‚ä½•éƒ¨ç½²æ¨¡å‹æ¨ç†æœåŠ¡ï¼Ÿ

### 0. ç»¼è¿°/èƒŒæ™¯

**(1) ä»»ä½•æ·±åº¦å­¦ä¹ æ¨ç†æœåŠ¡è§£å†³æ–¹æ¡ˆéƒ½éœ€è¦åº”å¯¹ä¸¤ä¸ªåŸºæœ¬æŒ‘æˆ˜ï¼š**

1. **ç®¡ç†å¤šä¸ªæ¨¡å‹ã€‚**

2. **ç‰ˆæœ¬æ§åˆ¶ã€åŠ è½½å’Œå¸è½½æ¨¡å‹ã€‚**

(2) éƒ¨ç½²ä¸€ä¸ª**é«˜æ€§èƒ½çš„ (Performant)**Â å’Œ**å¯æ‰©å±•çš„ (Scalable)**Â æµæ°´çº¿ (Pipeline)åŒ…æ‹¬å¦‚ä¸‹ 5 æ­¥ï¼š

1. é¢„å¤„ç† (Pre-process) åŸå§‹å›¾åƒ (Raw image)

2. æ£€æµ‹ (Detect) å›¾åƒä¸­åŒ…å«æ–‡å­—çš„éƒ¨åˆ† (Text Detection Model)

3. è£å‰ª (Crop) åŒ…å«æ–‡å­—çš„å›¾åƒåŒºåŸŸ

4. æŸ¥æ‰¾æ–‡æœ¬æ¦‚ç‡ (Text Recognition Model)

5. è½¬æ¢æ¦‚ç‡å€¼åˆ°å®é™…æ–‡æœ¬

(3) **éƒ¨ç½²å¤šä¸ªæ¨¡å‹**

- ç®¡ç†å¤šä¸ªæ¨¡å‹çš„å…³é”®æŒ‘æˆ˜æ˜¯ï¼Œå¦‚ä½•æ„å»ºä¸€ä¸ªèƒ½å¤Ÿæ»¡è¶³ (Cater to) ä¸åŒæ¨¡å‹çš„ä¸åŒéœ€æ±‚çš„åŸºç¡€è®¾æ–½ã€‚
  
  - ä¾‹å¦‚ï¼Œ**ç”¨æˆ·å¯èƒ½éœ€è¦åœ¨åŒä¸€å°æœåŠ¡å™¨ä¸ŠåŒæ—¶éƒ¨ç½²ä¸€ä¸ª PyTorch Model å’Œä¸€ä¸ª TensorFlow Model**ï¼Œ**è€Œä¸”æ¨¡å‹çš„è´Ÿè½½ (Have different loads) ä¹Ÿä¸åŒ**ï¼Œ**éœ€è¦åœ¨ä¸åŒçš„ç¡¬ä»¶è®¾å¤‡ (different devices) ä¸Šè¿è¡Œ**ï¼Œå¹¶ä¸”éœ€è¦**ç‹¬ç«‹ç®¡ç†æœåŠ¡é…ç½®ï¼ˆdifferent serving configurationsï¼Œæ¨¡å‹é˜Ÿåˆ— Model queuesã€ç‰ˆæœ¬ Versionsã€ç¼“å­˜ Cachingã€åŠ é€Ÿ Accelerationç­‰ï¼‰**ã€‚

- Triton Inference Server èƒ½å¤Ÿæ»¡è¶³ (Cater to) ä¸Šè¿°æ‰€æœ‰éœ€æ±‚ã€ç”šè‡³æ›´å¤šã€‚

### 1. å»ºä»“ã€æ’¸é…ç½®

0. ä½¿ç”¨ Triton Inference Server éƒ¨ç½²æ¨¡å‹çš„ç¬¬ä¸€æ­¥ï¼Œæ˜¯å»ºç«‹**ä¸€ä¸ªå­˜å‚¨è¿™äº›æ¨¡å‹ (Houes the models) çš„æ¨¡å‹ä»“åº“ (Model repository)**ï¼Œä»¥åŠ**ä¸€å †é…ç½® (Configuration schema)**ã€‚
   
   - ä¸ºäº†æ¼”ç¤ºï¼Œ**æˆ‘ä»¬å°†åˆ©ç”¨ä¸€ä¸ªæ–‡æœ¬æ£€æµ‹æ¨¡å‹** EASTÂ **å’Œä¸€ä¸ªæ–‡æœ¬è¯†åˆ«æ¨¡å‹**ã€‚è¿™ä¸ªå·¥ä½œæµç¨‹åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å¯¹OpenCV's Text Detectionä¾‹å­çš„æ”¹ç¼–ã€‚

1. é¦–å…ˆï¼Œè®©æˆ‘ä»¬å…‹éš†æŒ‡å—ä»“åº“ (https://github.com/triton-inference-server/tutorials) å¹¶å¯¼èˆªåˆ°è¯¥æ–‡ä»¶å¤¹ã€‚

```text
git clone https://github.com/triton-inference-server/tutorials.git
cd Conceptual_Guide/Part_1-model_deployment
```

2. æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸‹è½½å¿…è¦çš„æ¨¡å‹ï¼Œå¹¶ç¡®ä¿ä»–ä»¬æ˜¯ Triton å…¼å®¹çš„æ ¼å¼ (Model format)ã€‚

#### 1.1. ä¸‹è½½ Model 1: Text Detection

1. ä¸‹è½½å’Œè§£å‹ OpenCV çš„ EAST Model ã€‚

```text
wget https://www.dropbox.com/s/r2ingd0l3zt8hxs/frozen_east_text_detection.tar.gz
tar -xvf frozen_east_text_detection.tar.gz
```

#### 1.2. å¯¼å‡ºåˆ° ONNX æ ¼å¼ã€‚

> Note: ä¸‹ä¸€æ­¥éœ€è¦æ‚¨å®‰è£…å¥½ TensorFlow åº“ã€‚æˆ‘ä»¬å»ºè®®æ‚¨åœ¨Â **NGC TensorFlow å®¹å™¨ç¯å¢ƒ**ä¸­æ‰§è¡Œä¸‹ä¸€æ­¥ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è¯¥ç¯å¢ƒï¼š*docker run -it --gpus all -v ${PWD}:/workspaceÂ http://nvcr.io/nvidia/tensorflow:-tf2-py3*

```text
pip install -U tf2onnx
python -m tf2onnx.convert --input frozen_east_text_detection.pb --inputs "input_images:0" --outputs "feature_fusion/Conv_7/Sigmoid:0","feature_fusion/concat_3:0" --output detection.onnx
```

### 2. ä¸‹è½½Model 2: Text Recognition

- ä¸‹è½½æ–‡æœ¬è¯†åˆ«æ¨¡å‹æƒé‡ (Text Recognition Model Weights)ã€‚

```text
wget https://www.dropbox.com/sh/j3xmli4di1zuv3s/AABzCC1KGbIRe2wRwa3diWKwa/None-ResNet-None-CTC.pth
```

- å°†æ¨¡å‹å¯¼å‡ºä¸º .onnx æ–‡ä»¶ï¼Œä½¿ç”¨æ–‡ä»¶å¤¹ utils/ ä¸‹æ¨¡å‹å®šä¹‰æ–‡ä»¶ (Model definition file) ä¸­çš„ model.py æ–‡ä»¶ã€‚

> Note: ä»¥ä¸‹çš„ Python è„šæœ¬éœ€è¦æ‚¨å·²ç»å®‰è£… PyTorch åº“ã€‚æˆ‘ä»¬å»ºè®®æ‚¨åœ¨ NGC PyTorch å®¹å™¨ç¯å¢ƒä¸­æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨è¯¥ç¯å¢ƒï¼š*docker run -it --gpus all -v ${PWD}:/workspaceÂ http://nvcr.io/nvidia/pytorch:-py3*

```text
import torch
from utils.model import STRModel


# Create PyTorch Model Object
model = STRModel(input_channels=1, output_channels=512, num_classes=37)

# Load model weights from external file
state = torch.load("None-ResNet-None-CTC.pth")
state = {key.replace("module.", ""): value for key, value in state.items()}
model.load_state_dict(state)

# Create ONNX file by tracing model
trace_input = torch.randn(1, 1, 32, 100)
torch.onnx.export(model, trace_input, "str.onnx", verbose=True)
```

### 3. è®¾ç½®æ¨¡å‹ä»“åº“ (Model repository)

- æ¨¡å‹ä»“åº“ (Model repository) æ˜¯ Triton è¯»å–æ¨¡å‹åŠå…¶ç›¸å…³å…ƒæ•°æ®ï¼ˆMetadataï¼Œå¦‚é…ç½®ã€ç‰ˆæœ¬æ–‡ä»¶ç­‰ï¼‰çš„æ–¹å¼ã€‚

- **æ¨¡å‹ä»“åº“å¯ä»¥å­˜åœ¨äº**
  
  - **æœ¬åœ° (Local)**
  - **ç½‘ç»œè¿æ¥çš„æ–‡ä»¶ç³»ç»Ÿ (Network attached filesystem)**
  - **äº‘å¯¹è±¡å­˜å‚¨ (Cloud object store)**
    - åƒ AWS S3ã€Azure Blob Storage æˆ– Google Cloud Storage

- æœ‰å…³æ¨¡å‹ä»“åº“ä½ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[æ–‡æ¡£](https://link.zhihu.com/?target=https%3A//docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html%23model-repository-locations)ã€‚

- **æœåŠ¡å™¨ (Servers) è¿˜å¯ä»¥ä½¿ç”¨å¤šä¸ªä¸åŒçš„æ¨¡å‹ä»“åº“**ã€‚

- ä¸ºäº†ç®€å•èµ·è§ï¼Œæœ¬è¯´æ˜ä»…ä½¿ç”¨å­˜å‚¨åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿ (Local filesystem) ä¸­çš„å•ä¸ªä»“åº“ï¼Œç»“æ„å¦‚ä¸‹ï¼š

```text
# Example repository structure
<model-repository>/
  <model-name>/
    [config.pbtxt]
    [<output-labels-file> ...]
    <version>/
      <model-definition-file>
    <version>/
      <model-definition-file>
    ...
  <model-name>/
    [config.pbtxt]
    [<output-labels-file> ...]
    <version>/
      <model-definition-file>
    <version>/
      <model-definition-file>
    ...
  ...
```

ä¸Šé¢çš„æ¨¡å‹ä»“åº“ (Model repository) ç»“æ„ä¸­ï¼Œæœ‰ä¸‰ä¸ªé‡è¦ç»„æˆéƒ¨åˆ†ï¼š

1. **model-name: æ¨¡å‹çš„æ ‡è¯†ã€‚**

2. **config.pbtxt**: å¯¹äºæ¯ä¸ªæ¨¡å‹ï¼Œç”¨æˆ·å¯ä»¥å®šä¹‰ä¸€ä¸ªæ¨¡å‹é…ç½® (Model configuration)ã€‚è¿™ä¸ªé…ç½®è‡³å°‘éœ€è¦å®šä¹‰ï¼šåç«¯ï¼Œåç§°ï¼Œå½¢çŠ¶ï¼Œä»¥åŠæ¨¡å‹è¾“å…¥è¾“å‡ºçš„æ•°æ®ç±»å‹ (the backend, name, shape, and datatype of model inputs and outputs)ã€‚å¯¹äºå¤§å¤šæ•°æµè¡Œçš„åç«¯ (backend)ï¼Œè¿™ä¸ªé…ç½®æ–‡ä»¶ä¼šé»˜è®¤è‡ªåŠ¨ç”Ÿæˆã€‚é…ç½®æ–‡ä»¶çš„å®Œæ•´è§„èŒƒå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°Â [model_config_protobuf_definition](https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto)ã€‚

3. version: ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥ï¼Œèƒ½è®©åŒä¸€ä¸ªæ¨¡å‹çš„å¤šä¸ªç‰ˆæœ¬å¯ä¾›ä½¿ç”¨ï¼Œå–å†³äºä½ ä½¿ç”¨ä»€ä¹ˆç­–ç•¥ã€‚[æœ‰å…³ç‰ˆæœ¬æ§åˆ¶çš„æ›´å¤šä¿¡æ¯](https://link.zhihu.com/?target=https%3A//docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html%23model-versions)ã€‚  

åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæ‚¨å¯ä»¥è¿™æ ·è®¾ç½®æ¨¡å‹ä»“åº“ (Model repository) çš„ç»“æ„ï¼š

```text
mkdir -p model_repository/text_detection/1
mv detection.onnx model_repository/text_detection/1/model.onnx

mkdir -p model_repository/text_recognition/1
mv str.onnx model_repository/text_recognition/1/model.onnx
```

è¿™äº›å‘½ä»¤å°†ç»™æ‚¨ä¸€ä¸ªçœ‹èµ·æ¥å¦‚ä¸‹çš„å­˜å‚¨åº“ï¼š

```text
# Expected folder layout
model_repository/
â”œâ”€â”€ text_detection
â”‚   â”œâ”€â”€ 1
â”‚   â”‚   â””â”€â”€ model.onnx
â”‚   â””â”€â”€ config.pbtxt
â””â”€â”€ text_recognition
    â”œâ”€â”€ 1
    â”‚   â””â”€â”€ model.onnx
    â””â”€â”€ config.pbtxt
```

è¯·æ³¨æ„ï¼Œå¯¹äºè¿™ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å·²ç»åˆ›å»ºäº† config.pbtxt æ–‡ä»¶ï¼Œå¹¶å°†å®ƒä»¬æ”¾åœ¨äº†å¿…è¦çš„ä½ç½®ã€‚åœ¨ä¸‹ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†è®¨è®ºè¿™äº›æ–‡ä»¶çš„å†…å®¹ã€‚

### 4. æ¨¡å‹é…ç½® (Model configuration)

å‡†å¤‡å¥½äº†æ¨¡å‹å’Œæ–‡ä»¶ç»“æ„ä¹‹åï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥éœ€è¦æŸ¥çœ‹çš„æ˜¯ config.pbtxt æ¨¡å‹é…ç½®æ–‡ä»¶ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬çœ‹ä¸‹ EAST text detection æ¨¡å‹çš„æ¨¡å‹é…ç½®ï¼Œä½äº /model_repository/text_detection/config.pbtxt ã€‚è¿™æ˜¾ç¤ºäº† text_detection æ˜¯ä¸€ä¸ª ONNX æ¨¡å‹ï¼Œæœ‰ 1 ä¸ª Input å’Œ 2 ä¸ª Output tensorsã€‚

```text
name: "text_detection"
backend: "onnxruntime"
max_batch_size : 256
input [
  {
    name: "input_images:0"
    data_type: TYPE_FP32
    dims: [ -1, -1, -1, 3 ]
  }
]
output [
  {
    name: "feature_fusion/Conv_7/Sigmoid:0"
    data_type: TYPE_FP32
    dims: [ -1, -1, -1, 1 ]
  }
]
output [
  {
    name: "feature_fusion/concat_3:0"
    data_type: TYPE_FP32
    dims: [ -1, -1, -1, 5 ]
  }
]
```

1. name: â€œnameâ€æ˜¯ä¸€ä¸ªå¯é€‰å­—æ®µï¼Œå…¶å€¼åº”ä¸æ¨¡å‹ç›®å½•çš„åç§°ç›¸åŒ¹é…ã€‚

2. backend: æ­¤å­—æ®µè¡¨ç¤ºä½¿ç”¨å“ªä¸ªåç«¯æ¥è¿è¡Œæ¨¡å‹ã€‚Triton æ”¯æŒå„ç§åç«¯ï¼Œä¾‹å¦‚ TensorFlowã€PyTorchã€Pythonã€ONNX ç­‰ç­‰ã€‚æœ‰å…³å­—æ®µé€‰æ‹©çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚è€ƒè¿™äº›[æ³¨é‡Š](https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/backend%23backends)ã€‚

3. max_batch_size: å¦‚å…¶åï¼Œè¯¥å­—æ®µç”¨äºå®šä¹‰æ¨¡å‹æœ€å¤šèƒ½æ”¯æŒçš„æ‰¹å¤„ç†å¤§å°ã€‚

4. input and output: è¾“å…¥å’Œè¾“å‡ºéƒ¨åˆ†æŒ‡å®šäº†åç§°ã€å½¢çŠ¶ã€æ•°æ®ç±»å‹ç­‰ä¿¡æ¯ï¼ŒåŒæ—¶æä¾›äº†Â [reshaping](https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md%23reshape)Â å’ŒÂ [ragged batches](https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/server/blob/main/docs/user_guide/ragged_batching.md%23ragged-batching)ï¼ˆåŠ¨æ€ Batchï¼‰ ç­‰æ“ä½œçš„æ”¯æŒã€‚  

å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå¯ä»¥çœç•¥è¾“å…¥å’Œè¾“å‡ºéƒ¨åˆ†ï¼Œè®© Triton ç›´æ¥ä»æ¨¡å‹æ–‡ä»¶ä¸­æå–è¿™äº›ä¿¡æ¯ã€‚è¿™é‡Œæˆ‘ä»¬åŒ…å«å®ƒä»¬ï¼Œæ˜¯ä¸ºäº†æ¸…æ™°èµ·è§ï¼Œä¹Ÿå› ä¸ºä»¥ååœ¨å®¢æˆ·ç«¯åº”ç”¨ç¨‹åº (in the client applications) ä¸­éœ€è¦çŸ¥é“è¾“å‡ºå¼ é‡çš„åç§°ã€‚

è¦è·å–æ‰€æœ‰æ”¯æŒçš„å­—æ®µï¼ŒåŠå…¶å€¼çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…Â [model config protobuf definition file](https://link.zhihu.com/?target=https%3A//github.com/triton-inference-server/common/blob/main/protobuf/model_config.proto)ã€‚

# MPI ä¸ NCCL è¯´æ˜

## 1. æ¦‚è¿°

**(1) MPI**

MPIï¼Œå…¨ç§°ä¸ºæ¶ˆæ¯ä¼ é€’æ¥å£ï¼ˆMessage Passing Interfaceï¼‰ï¼Œæ˜¯ä¸€ä¸ªæ ‡å‡†åŒ–å’Œä¾¿æºå¼çš„æ¶ˆæ¯ä¼ é€’ç³»ç»Ÿï¼Œè®¾è®¡ç”¨äºå¹¶è¡Œè®¡ç®—ä¸­çš„è¿›ç¨‹é—´é€šä¿¡ã€‚MPI æä¾›äº†ä¸€ç§åœ¨åˆ†å¸ƒå¼å†…å­˜ç³»ç»Ÿä¸­è¿›è¡Œè¿›ç¨‹é—´é€šä¿¡çš„æ–¹æ³•ï¼Œè¿™æ˜¯å¹¶è¡Œè®¡ç®—çš„ä¸€ç§å¸¸è§æ¨¡å¼ã€‚åœ¨è¿™ç§æ¨¡å¼ä¸­ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æœ‰è‡ªå·±çš„ç§æœ‰å†…å­˜ï¼Œè¿›ç¨‹ä¹‹é—´çš„é€šä¿¡éœ€è¦é€šè¿‡æ¶ˆæ¯ä¼ é€’æ¥å®Œæˆã€‚

**(2)  NCCL**

NCCLæ˜¯Nvidia Collective multi-GPU Communication Libraryçš„ç®€ç§°ï¼Œå®ƒæ˜¯ä¸€ä¸ªå®ç°å¤šGPUçš„collective communicationé€šä¿¡ï¼ˆall-gather, reduce, broadcastï¼‰åº“ï¼ŒNvidiaåšäº†å¾ˆå¤šä¼˜åŒ–ï¼Œä»¥åœ¨PCIeã€Nvlinkã€InfiniBandä¸Šå®ç°è¾ƒé«˜çš„é€šä¿¡é€Ÿåº¦ã€‚

## 2. NCCL

NCCLå®ç°æˆCUDA C++ kernelsï¼ŒåŒ…å«3ç§primitive operationsï¼š Copyï¼ŒReduceï¼ŒReduceAndCopyã€‚ç›®å‰NCCL 1.0ç‰ˆæœ¬åªæ”¯æŒå•æœºå¤šå¡ï¼Œå¡ä¹‹é—´é€šè¿‡PCIeã€NVlinkã€GPU Direct P2Pæ¥é€šä¿¡ã€‚NCCL 2.0ä¼šæ”¯æŒå¤šæœºå¤šå¡ï¼Œå¤šæœºé—´é€šè¿‡Sockets (Ethernet)æˆ–è€…InfiniBand with GPU Direct RDMAé€šä¿¡ã€‚

- å¹¶è¡Œä»»åŠ¡çš„é€šä¿¡ä¸€èˆ¬å¯ä»¥åˆ†ä¸ºï¼š
  
  - Point-to-point communication
  
  - Collective communicationã€‚

- P2Pé€šä¿¡è¿™ç§æ¨¡å¼åªæœ‰ä¸€ä¸ªsenderå’Œä¸€ä¸ªreceiverï¼Œå®ç°èµ·æ¥æ¯”è¾ƒç®€å•ã€‚

- Collective communicationåŒ…å«å¤šä¸ªsenderå¤šä¸ªreceiverï¼Œä¸€èˆ¬çš„é€šä¿¡åŸè¯­åŒ…æ‹¬ï¼š
  
  - broadcastgather
  
  - all-gather
  
  - scatter
  
  - reduceï¼šä»å¤šä¸ªsenderé‚£é‡Œæ¥æ”¶æ•°æ®ï¼Œæœ€ç»ˆcombineåˆ°ä¸€ä¸ªèŠ‚ç‚¹ä¸Šã€‚
  
  - all-reduceï¼šä»å¤šä¸ªsenderé‚£é‡Œæ¥æ”¶æ•°æ®ï¼Œæœ€ç»ˆcombineåˆ°æ¯ä¸€ä¸ªèŠ‚ç‚¹ä¸Šã€‚
  
  - reduce-scatter
  
  - all-to-all

è€Œä¼ ç»ŸCollective communicationå‡è®¾é€šä¿¡èŠ‚ç‚¹ç»„æˆçš„topologyæ˜¯ä¸€é¢—fat treeï¼Œè¿™æ ·é€šä¿¡æ•ˆç‡æœ€é«˜ã€‚ä½†å®é™…çš„é€šä¿¡topologyå¯èƒ½æ¯”è¾ƒå¤æ‚ï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªfat treeã€‚å› æ­¤ä¸€èˆ¬ç”¨ring-based Collective communicationã€‚

ring-base collectiveså°†æ‰€æœ‰çš„é€šä¿¡èŠ‚ç‚¹é€šè¿‡é¦–å°¾è¿æ¥å½¢æˆä¸€ä¸ªå•å‘ç¯ï¼Œæ•°æ®åœ¨ç¯ä¸Šä¾æ¬¡ä¼ è¾“ã€‚ä»¥broadcastä¸ºä¾‹ï¼Œ å‡è®¾æœ‰4ä¸ªGPUï¼ŒGPU0ä¸ºsenderå°†ä¿¡æ¯å‘é€ç»™å‰©ä¸‹çš„GPUï¼ŒæŒ‰ç…§ç¯çš„æ–¹å¼ä¾æ¬¡ä¼ è¾“ï¼ŒGPU0-->GPU1-->GPU2-->GPU3ï¼Œè‹¥æ•°æ®é‡ä¸ºNï¼Œå¸¦å®½ä¸ºBï¼Œæ•´ä¸ªä¼ è¾“æ—¶é—´ä¸ºï¼ˆK-1ï¼‰N/Bã€‚æ—¶é—´éšç€èŠ‚ç‚¹æ•°çº¿æ€§å¢é•¿ï¼Œä¸æ˜¯å¾ˆé«˜æ•ˆã€‚

GPU1æ¥æ”¶åˆ°GPU0çš„ä¸€ä»½æ•°æ®åï¼Œä¹Ÿæ¥ç€ä¼ åˆ°ç¯çš„ä¸‹ä¸ªèŠ‚ç‚¹ï¼Œè¿™æ ·ä»¥æ­¤ç±»æ¨ï¼Œæœ€åèŠ±çš„æ—¶é—´ä¸º

S*(N/S/B) + (k-2)*(N/S/B) = N(S+K-2)/(SB) --> N/Bï¼Œæ¡ä»¶æ˜¯Sè¿œå¤§äºKï¼Œå³æ•°æ®çš„ä»½æ•°å¤§äºèŠ‚ç‚¹æ•°ï¼Œè¿™ä¸ªå¾ˆå®¹æ˜“æ»¡è¶³ã€‚æ‰€ä»¥é€šä¿¡æ—¶é—´ä¸éšèŠ‚ç‚¹æ•°çš„å¢åŠ è€Œå¢åŠ ï¼Œåªå’Œæ•°æ®æ€»é‡ä»¥åŠå¸¦å®½æœ‰å…³ã€‚å…¶å®ƒé€šä¿¡æ“ä½œæ¯”å¦‚reduceã€gatherä»¥æ­¤ç±»æ¨ã€‚

é‚£ä¹ˆåœ¨ä»¥GPUä¸ºé€šä¿¡èŠ‚ç‚¹çš„åœºæ™¯ä¸‹ï¼Œæ€ä¹ˆæ„å»ºé€šä¿¡ç¯å‘¢ï¼Ÿ

# Triton23.10éƒ¨ç½²TensorRT-LLM

### 0. æ¦‚è¿°

TensorRT-LLMå‘å¸ƒäº†ï¼Œç»§æ‰¿è‡ªfastertransformerï¼Œæ˜¯å¤§è¯­è¨€ç‰ˆæœ¬çš„TensorRTï¼Œä¾èµ–TensorRT9.xç‰ˆæœ¬å»è·‘ã€‚ä¸»è¦ä»‹ç»ä¸‹æ–°ç‰¹æ€§ï¼š

- ç‰¹æ®Šç‰ˆæœ¬çš„trtï¼Œ**æœ‰äº›æ¶æ„ä¸æ”¯æŒï¼Œæ¯”å¦‚å›¾çµ**ï¼ˆæ¥æº NVIDIA AI Inference Day å¤§æ¨¡å‹æ¨ç†çº¿ä¸Šç ”è®¨ä¼šï¼‰ï¼Œå¦å¤–Torch-TensorRTç›®å‰ç»§ç»­ä½¿ç”¨trt-8.6ï¼Œä¹‹åè¿ç§»åˆ°trt-10.0
- å†™TensorRT-pluginæ›´æ–¹ä¾¿äº†ï¼Œå¯ä»¥ä½¿ç”¨pythonå†™plugin
- æ›´æ–°æ”¯æŒSDå’ŒNLPç›¸å…³æ¨¡å‹çš„æ”¯æŒå’Œä¼˜åŒ–ï¼Œä¸æ—¶ä¿±è¿›

### 1. å®‰è£…docker-ce å’Œ nvidia-container-toolkit

```bash
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
     && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
     && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
           sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
           sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
apt update
apt install nvidia-container-toolkit
```

### 2. æ‹‰å–é•œåƒ

```bash
docker pull nvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3

# è¿è¡Œé•œåƒ
docker run --gpus all -itd --network=host \
-v /sda/AIRepo/TRTDir:/TRTDir \
--name triton-trtllm \
nvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3
# docker run --gpus all -itd --network=host \
# -v /sda/AIRepo/TRTDir:/TRTDir \
# --name triton-trtllm \
# --ulimit core=0 --ulimit memlock=-1 --ulimit stack=67108864 \
# --shm-size=100G \
# nvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3
```

### 3. é…ç½®é•œåƒ

```bash
cd /TRTDir
# æ·»åŠ git-lfsä»“åº“å¹¶ä¸‹è½½
curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh |  bash -
apt update
apt install -y git git-lfs
# å®‰è£…cmake
apt install -y cmake
pip3 install cmake

# ä¸‹è½½chatglm2-6b Hugging Face é¢„è®­ç»ƒæ¨¡å‹
# ps autodl åŠ é€Ÿ source /etc/network_turbo
git clone https://huggingface.co/THUDM/chatglm2-6b
cd chatglm2-6b
git lfs install
git lfs pull
# ä¸‹è½½ TensorRT-LLM æºä»£ç ç”¨äºç¼–è¯‘
git clone https://github.com/NVIDIA/TensorRT-LLM.git
cd TensorRT-LLM
git submodule update --init --recursive
git lfs install
git lfs pull


# æ¢æº
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple

# å®‰è£…ä¾èµ–ï¼ˆéœ€è¿æ¥ç½‘ç»œï¼‰
pip install -e /TRTDir/TensorRT-LLM
# å®‰è£…ä¾èµ–pythonåŒ…ï¼ˆéœ€è¿æ¥ç½‘ç»œï¼‰å¹¶ç¼–è¯‘ï¼ˆ6æ ¸12çº¿ç¨‹è€—æ—¶58minï¼‰
python3 ./scripts/build_wheel.py --trt_root /usr/local/tensorrt
# å®‰è£…ç¼–è¯‘å¥½çš„pythonåŒ…
pip install ./build/tensorrt_llm*.whl
# é€€å‡ºå®¹å™¨å¹¶commitä¸ºé•œåƒä¿å­˜ä¸Šä¸‹æ–‡
```

### æ„å»ºChatGLM2 engine

```bash
python3 build.py \
--model_dir=/TRTDir/chatglm2-6b  \
--dtype=float16 \
--use_gpt_attention_plugin=float16 \
--use_gemm_plugin=float16

python3 build.py \
--model_dir=/TRTDir/chatglm2-6b \
--dtype=float16 \
--use_gpt_attention_plugin=float16 \
--use_gemm_plugin=float16 \
--output_dir=trtModel2 \
--use_weight_only \
--weight_only_precision=int8

python run.py --engine_dir=trtModel2
```

### å¹¶å‘

```bash
python3 build.py --model_name=chatglm2_6b 
                 --model_dir=/root/autodl-tmp/chatglm2-6b 
                 --output_dir=trtModel 
                 --use_weight_only 
                 --weight_only_precision=int8 
                 --tp_size=2 --world_size=2

mpirun --allow-run-as-root -n 2 python run.py 
       --engine_dir=trtModel 
       --model_name=chatglm2_6b 
       --tokenizer_dir=/root/autodl-tmp/chatglm2-6b  
       --input_text="Hello tell me about Iphone"
```

### åƒé—®7B

```bash
# æ‹‰å–é•œåƒå¹¶è¿è¡Œ
docker run -d \    
    --name triton2 \
    --net host \
    --shm-size=2g \
    --ulimit memlock=-1 \
    --ulimit stack=67108864 \
    --gpus all \
    -v ${PWD}/tensorrtllm_backend:/tensorrtllm_backend \
    #-v ${PWD}/Qwen-7B-Chat-TensorRT-LLM/qwen:/root/qwen \
    -v ${PWD}/Qwen-7B-Chat-TensorRT-LLM/chatglm2-6b:/root/chatglm2-6b \
    -v ${PWD}/TensorRT-LLM-build:/TensorRT-LLM-build \
    nvcr.io/nvidia/tritonserver:23.10-trtllm-python-py3 sleep 864000

# è¿›å…¥å®¹å™¨ï¼Œå®‰è£…git-lfs
apt update
apt install git-lfs

# å®‰è£…TensorRT-LLM pythonç‰ˆï¼Œæ–¹ä¾¿å¾…ä¼šç¼–è¯‘Engine
# pip install git+https://github.com/NVIDIA/TensorRT-LLM.git
pip install -e  /TensorRT-LLM-build

# å¤åˆ¶libåº“è¿‡å»ï¼Œå¦åˆ™æ— æ³•è¿è¡Œ
mkdir /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/
cp /opt/tritonserver/backends/tensorrtllm/* /usr/local/lib/python3.10/dist-packages/tensorrt_llm/libs/

# è¿›å…¥qwenç›®å½•
cd /root/qwen
# å®‰è£…ä¾èµ–
pip install -r /root/qwen/requirements.txt
# è½¬smooth int8 æƒé‡ï¼ˆå¯é€‰ï¼‰
# å°†Huggingfaceæ ¼å¼çš„æ•°æ®è½¬æˆFT(FastTransformer)éœ€è¦çš„æ•°æ®æ ¼å¼
python3 /root/qwen/hf_qwen_convert.py # ä¸è¦ç”¨ --smoothquant=0.5
# ç¼–è¯‘
# qwen
python3 build.py --use_weight_only --weight_only_precision=int8
# chatgml2-6b
python3 build.py --model_dir=./chatglm2-6b --dtype float16 --use_gpt_attention_plugin float16 --use_gemm_plugin float16
# è¿è¡Œæµ‹è¯•
python3 run.py
```

### å‚è€ƒæ–‡çŒ®

https://zhuanlan.zhihu.com/p/668548188

https://www.http5.cn/index.php/archives/55/

https://huggingface.co/THUDM/chatglm2-6b

[GitHub - triton-inference-server/tensorrtllm_backend: The Triton TensorRT-LLM Backend](https://github.com/triton-inference-server/tensorrtllm_backend)

https://ai.oldpan.me/t/topic/260

https://zhuanlan.zhihu.com/p/663338695

https://github.com/Tlntin/Qwen-7B-Chat-TensorRT-LLM

https://zhuanlan.zhihu.com/p/664545577

Triton

https://zhuanlan.zhihu.com/p/660990715

åˆ†å¸ƒå¼æ¨ç†

https://ai.oldpan.me/t/topic/172

https://www.zhihu.com/question/63219175/answer/206697974

https://ai.oldpan.me/t/topic/177

https://ai.oldpan.me/t/topic/199

https://www.bilibili.com/video/BV1h44y1c72B/?spm_id_from=333.788&vd_source=eec038509607175d58cdfe2e824e8ba2

[å¤§å¤§å¤§å¤§å¤§æ¨¡å‹éƒ¨ç½²æ–¹æ¡ˆæŠ›ç –å¼•ç‰ - AIå¤§æ¨¡å‹ - è€æ½˜çš„AIç¤¾åŒº](https://ai.oldpan.me/t/topic/118)

https://juejin.cn/post/7219245946739179578

https://zhuanlan.zhihu.com/p/626008090

åˆ†å¸ƒå¼æ¨ç†-å¤§æ¨¡å‹æ¨ç†æ¡†æ¶-ç»¼è¿°

https://zhuanlan.zhihu.com/p/665089816

é•œåƒç¼–è¯‘

https://zhuanlan.zhihu.com/p/663915644

Pytorchæ•™ç¨‹

[9.1 ä½¿ç”¨ONNXè¿›è¡Œéƒ¨ç½²å¹¶æ¨ç† &#8212; æ·±å…¥æµ…å‡ºPyTorch](https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%B9%9D%E7%AB%A0/9.1%20%E4%BD%BF%E7%94%A8ONNX%E8%BF%9B%E8%A1%8C%E9%83%A8%E7%BD%B2%E5%B9%B6%E6%8E%A8%E7%90%86.html)

æ·±åº¦å­¦ç†è®ºåŸºç¡€

[Attention æ³¨æ„åŠ›æœºåˆ¶ | é²è€å¸ˆ](https://lulaoshi.info/deep-learning/attention/attention.html#attention%E6%9C%BA%E5%88%B6)

ç¼–è¯‘å™¨ã€æ¨¡å‹ä¼˜åŒ–åŠAIç›¸å…³åšå®¢

http://giantpandacv.com/project/%E9%83%A8%E7%BD%B2%E4%BC%98%E5%8C%96/  

vLLM

[ä½¿ç”¨Dockerã€vllmå’ŒGradioéƒ¨ç½²å¼€æºLLMï¼Œä»¥Qwen-7B-Chatä¸ºä¾‹ | LittleFishâ€™Blog](https://www.xiaoiluo.com/article/vllm-docker-server)

ONNX

https://zhuanlan.zhihu.com/p/453084182

https://zhuanlan.zhihu.com/p/641975976
